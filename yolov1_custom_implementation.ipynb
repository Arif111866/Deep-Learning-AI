{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arif111866/Deep-Learning-AI/blob/main/yolov1_custom_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "2yANCW18assa"
      },
      "id": "2yANCW18assa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "595536a3",
      "metadata": {
        "id": "595536a3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6384ee7a",
      "metadata": {
        "id": "6384ee7a"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Define the YOLOv1 network\n",
        "class YOLOv1(nn.Module):\n",
        "    def __init__(self, S=7, B=2, C=1):\n",
        "        super(YOLOv1, self).__init__()\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.C = C\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(192, 128, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1024 * S * S, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(4096, S * S * (B * 5 + C))\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.features(x).view(-1, self.S, self.S, self.B * 5 + self.C)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "model = YOLOv1()\n",
        "summary(model, (3, 224, 224)) # Assuming input image size is 3x224x224"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWD5v8HcaTr8",
        "outputId": "9e4f3672-4803-4c52-ce80-b057ecf8da2e"
      },
      "id": "pWD5v8HcaTr8",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,472\n",
            "              ReLU-2         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-3           [-1, 64, 56, 56]               0\n",
            "            Conv2d-4          [-1, 192, 56, 56]         110,784\n",
            "              ReLU-5          [-1, 192, 56, 56]               0\n",
            "         MaxPool2d-6          [-1, 192, 28, 28]               0\n",
            "            Conv2d-7          [-1, 128, 28, 28]          24,704\n",
            "              ReLU-8          [-1, 128, 28, 28]               0\n",
            "            Conv2d-9          [-1, 256, 28, 28]         295,168\n",
            "             ReLU-10          [-1, 256, 28, 28]               0\n",
            "        MaxPool2d-11          [-1, 256, 14, 14]               0\n",
            "           Conv2d-12          [-1, 256, 14, 14]         590,080\n",
            "             ReLU-13          [-1, 256, 14, 14]               0\n",
            "           Conv2d-14          [-1, 512, 14, 14]       1,180,160\n",
            "             ReLU-15          [-1, 512, 14, 14]               0\n",
            "        MaxPool2d-16            [-1, 512, 7, 7]               0\n",
            "           Conv2d-17            [-1, 512, 7, 7]       2,359,808\n",
            "             ReLU-18            [-1, 512, 7, 7]               0\n",
            "           Conv2d-19           [-1, 1024, 7, 7]       4,719,616\n",
            "             ReLU-20           [-1, 1024, 7, 7]               0\n",
            "           Conv2d-21           [-1, 1024, 7, 7]       9,438,208\n",
            "             ReLU-22           [-1, 1024, 7, 7]               0\n",
            "           Conv2d-23           [-1, 1024, 7, 7]       9,438,208\n",
            "             ReLU-24           [-1, 1024, 7, 7]               0\n",
            "          Flatten-25                [-1, 50176]               0\n",
            "           Linear-26                 [-1, 4096]     205,524,992\n",
            "             ReLU-27                 [-1, 4096]               0\n",
            "          Dropout-28                 [-1, 4096]               0\n",
            "           Linear-29                  [-1, 539]       2,208,283\n",
            "================================================================\n",
            "Total params: 235,899,483\n",
            "Trainable params: 235,899,483\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 34.74\n",
            "Params size (MB): 899.89\n",
            "Estimated Total Size (MB): 935.20\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e620c5c0",
      "metadata": {
        "id": "e620c5c0"
      },
      "outputs": [],
      "source": [
        "\n",
        "class WiderFaceDataset(Dataset):\n",
        "    def __init__(self, img_dir, label_dir, S=7, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.transform = transform\n",
        "        self.S = S\n",
        "        self.img_files = [f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.img_files[idx]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        label_path = os.path.join(self.label_dir, os.path.splitext(img_name)[0] + \".txt\")\n",
        "\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        h, w = img.size\n",
        "        target = torch.zeros((self.S, self.S, 5 * 2 + 1))  # B=2, C=1\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, \"r\") as f:\n",
        "                for line in f:\n",
        "                    cls, x_center, y_center, width, height = map(float, line.strip().split())\n",
        "                    grid_x = min(int(x_center * self.S), self.S - 1)\n",
        "                    grid_y = min(int(y_center * self.S), self.S - 1)\n",
        "                    # Assign to first predictor (simplified)\n",
        "                    target[grid_y, grid_x, 0] = x_center * self.S - grid_x\n",
        "                    target[grid_y, grid_x, 1] = y_center * self.S - grid_y\n",
        "                    target[grid_y, grid_x, 2] = width * self.S\n",
        "                    target[grid_y, grid_x, 3] = height * self.S\n",
        "                    target[grid_y, grid_x, 4] = 1.0  # Confidence\n",
        "                    target[grid_y, grid_x, 10] = cls\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b23918f",
      "metadata": {
        "id": "6b23918f"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def yolo_loss(preds, targets, S=7, B=2, lambda_coord=5.0, lambda_noobj=0.5):\n",
        "    batch_size = preds.size(0)\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        pred = preds[i].view(S, S, B * 5 + 1)  # [S, S, B*5 + C]\n",
        "        target = targets[i].view(S, S, B * 5 + 1)\n",
        "\n",
        "        for j in range(S):\n",
        "            for k in range(S):\n",
        "                # Objectness score (confidence)\n",
        "                obj_mask = target[j, k, 4] > 0  # True if object exists\n",
        "                noobj_mask = ~obj_mask\n",
        "\n",
        "                if obj_mask:\n",
        "                    # Coordinate loss (x, y, w, h) for the best bounding box\n",
        "                    best_iou = 0\n",
        "                    best_box_idx = 0\n",
        "                    for b in range(B):\n",
        "                        box_pred = pred[j, k, 5 * b:5 * (b + 1)]\n",
        "                        x, y, w, h, conf = box_pred\n",
        "                        target_box = target[j, k, :5]\n",
        "                        iou = calculate_iou((x, y, w, h), (target_box[0], target_box[1], target_box[2], target_box[3]))\n",
        "                        if iou > best_iou:\n",
        "                            best_iou = iou\n",
        "                            best_box_idx = b\n",
        "\n",
        "                    best_pred = pred[j, k, 5 * best_box_idx:5 * (best_box_idx + 1)]\n",
        "                    target_box = target[j, k, :5]\n",
        "                    coord_loss = nn.MSELoss()(best_pred[:4], target_box[:4]) * lambda_coord\n",
        "                    conf_loss = nn.MSELoss()(best_pred[4], target_box[4])\n",
        "                    class_loss = nn.MSELoss()(pred[j, k, 10], target[j, k, 10])\n",
        "\n",
        "                    total_loss += coord_loss + conf_loss + class_loss\n",
        "                # No object loss\n",
        "                for b in range(B):\n",
        "                    noobj_conf = pred[j, k, 5 * b + 4]\n",
        "                    zero_tensor = torch.tensor(0.0, device=pred.device)\n",
        "                    total_loss += nn.MSELoss()(noobj_conf, zero_tensor) * lambda_noobj * noobj_mask.float()\n",
        "\n",
        "    return total_loss / batch_size\n",
        "\n",
        "def calculate_iou(box1, box2):\n",
        "    # box: (x_center, y_center, width, height)\n",
        "    x1, y1, w1, h1 = box1\n",
        "    x2, y2, w2, h2 = box2\n",
        "    w1_half = w1 / 2\n",
        "    h1_half = h1 / 2\n",
        "    w2_half = w2 / 2\n",
        "    h2_half = h2 / 2\n",
        "\n",
        "    x1_min = x1 - w1_half\n",
        "    y1_min = y1 - h1_half\n",
        "    x1_max = x1 + w1_half\n",
        "    y1_max = y1 + h1_half\n",
        "\n",
        "    x2_min = x2 - w2_half\n",
        "    y2_min = y2 - h2_half\n",
        "    x2_max = x2 + w2_half\n",
        "    y2_max = y2 + h2_half\n",
        "\n",
        "    inter_x_min = max(x1_min, x2_min)\n",
        "    inter_y_min = max(y1_min, y2_min)\n",
        "    inter_x_max = min(x1_max, x2_max)\n",
        "    inter_y_max = min(y1_max, y2_max)\n",
        "\n",
        "    inter_area = max(0, inter_x_max - inter_x_min) * max(0, inter_y_max - inter_y_min)\n",
        "    union_area = w1 * h1 + w2 * h2 - inter_area\n",
        "\n",
        "    return inter_area / union_area if union_area > 0 else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b97c062",
      "metadata": {
        "id": "6b97c062"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Hyperparameters\n",
        "S = 7\n",
        "B = 2\n",
        "C = 1\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "batch_size = 16\n",
        "image_size = 224\n",
        "\n",
        "# Transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Dataset and DataLoader\n",
        "train_img_dir =   \"/home/cse/Desktop/Arif_30/dataset/WiderFace/archive/WIDER Face Dataset For YOLOv12/WIDER Face Dataset For YOLOv12/train/images\"  # Adjust path\n",
        "train_label_dir = \"/home/cse/Desktop/Arif_30/dataset/WiderFace/archive/WIDER Face Dataset For YOLOv12/WIDER Face Dataset For YOLOv12/train/labels\"  # Adjust path\n",
        "val_img_dir =     \"/home/cse/Desktop/Arif_30/dataset/WiderFace/archive/WIDER Face Dataset For YOLOv12/WIDER Face Dataset For YOLOv12/val/images\"  # Adjust path\n",
        "val_label_dir =   \"/home/cse/Desktop/Arif_30/dataset/WiderFace/archive/WIDER Face Dataset For YOLOv12/WIDER Face Dataset For YOLOv12/val/labels\"  # Adjust paths\n",
        "\n",
        "train_dataset = WiderFaceDataset(train_img_dir, train_label_dir, S=S, transform=transform)\n",
        "val_dataset = WiderFaceDataset(val_img_dir, val_label_dir, S=S, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = YOLOv1(S=S, B=B, C=C).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Metric\n",
        "metric = MeanAveragePrecision().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97197e31",
      "metadata": {
        "id": "97197e31",
        "outputId": "7f9f9656-31b9-4b80-cb4c-9c4705f0a8ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Step [0/805], Loss: 16.7158\n",
            "Epoch [1/10], Step [10/805], Loss: 9.6947\n",
            "Epoch [1/10], Step [20/805], Loss: 9.4927\n",
            "Epoch [1/10], Step [30/805], Loss: 6.8855\n",
            "Epoch [1/10], Step [40/805], Loss: 12.5985\n",
            "Epoch [1/10], Step [50/805], Loss: 11.7831\n",
            "Epoch [1/10], Step [60/805], Loss: 7.9501\n",
            "Epoch [1/10], Step [70/805], Loss: 10.0053\n",
            "Epoch [1/10], Step [80/805], Loss: 10.0564\n",
            "Epoch [1/10], Step [90/805], Loss: 8.4344\n",
            "Epoch [1/10], Step [100/805], Loss: 8.7301\n",
            "Epoch [1/10], Step [110/805], Loss: 8.1152\n",
            "Epoch [1/10], Step [120/805], Loss: 6.2247\n",
            "Epoch [1/10], Step [130/805], Loss: 9.7800\n",
            "Epoch [1/10], Step [140/805], Loss: 6.1475\n",
            "Epoch [1/10], Step [150/805], Loss: 6.5112\n",
            "Epoch [1/10], Step [160/805], Loss: 6.0331\n",
            "Epoch [1/10], Step [170/805], Loss: 9.4308\n",
            "Epoch [1/10], Step [180/805], Loss: 6.8759\n",
            "Epoch [1/10], Step [190/805], Loss: 8.6972\n",
            "Epoch [1/10], Step [200/805], Loss: 8.5525\n",
            "Epoch [1/10], Step [210/805], Loss: 8.0714\n",
            "Epoch [1/10], Step [220/805], Loss: 6.3439\n",
            "Epoch [1/10], Step [230/805], Loss: 10.3800\n",
            "Epoch [1/10], Step [240/805], Loss: 7.9868\n",
            "Epoch [1/10], Step [250/805], Loss: 6.9925\n",
            "Epoch [1/10], Step [260/805], Loss: 9.2631\n",
            "Epoch [1/10], Step [270/805], Loss: 5.6346\n",
            "Epoch [1/10], Step [280/805], Loss: 9.5895\n",
            "Epoch [1/10], Step [290/805], Loss: 6.4129\n",
            "Epoch [1/10], Step [300/805], Loss: 8.0936\n",
            "Epoch [1/10], Step [310/805], Loss: 7.5759\n",
            "Epoch [1/10], Step [320/805], Loss: 10.5960\n",
            "Epoch [1/10], Step [330/805], Loss: 5.0838\n",
            "Epoch [1/10], Step [340/805], Loss: 7.1984\n",
            "Epoch [1/10], Step [350/805], Loss: 11.4269\n",
            "Epoch [1/10], Step [360/805], Loss: 9.0223\n",
            "Epoch [1/10], Step [370/805], Loss: 11.6521\n",
            "Epoch [1/10], Step [380/805], Loss: 7.7914\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_idx, (images, targets) in enumerate(train_loader):\n",
        "        images, targets = images.to(device), targets.to(device)  # Move both images and targets to device\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = yolo_loss(outputs, targets, S=S, B=B)  # Ensure yolo_loss handles device tensors\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "torch.save(model.state_dict(), \"yolov1_widerface_model.pth\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}