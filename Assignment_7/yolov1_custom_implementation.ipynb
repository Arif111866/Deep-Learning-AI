{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arif111866/Deep-Learning-AI/blob/main/yolov1_custom_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MYV-DNoM00bp",
        "outputId": "5e1aa054-a20d-45fa-dfd6-b37dd1e0ee2c"
      },
      "id": "MYV-DNoM00bp",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.7.3-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.6.0+cu124)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.7.3-py3-none-any.whl (962 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.6/962.6 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.14.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchmetrics-1.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "595536a3",
      "metadata": {
        "id": "595536a3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6384ee7a",
      "metadata": {
        "id": "6384ee7a"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Define the YOLOv1 network\n",
        "class YOLOv1(nn.Module):\n",
        "    def __init__(self, S=7, B=2, C=1):\n",
        "        super(YOLOv1, self).__init__()\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.C = C\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(192, 128, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1024 * S * S, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(4096, S * S * (B * 5 + C))\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.features(x).view(-1, self.S, self.S, self.B * 5 + self.C)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary as torchsummary_summary\n",
        "model = YOLOv1()\n",
        "torchsummary_summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6hUIyj207X1",
        "outputId": "6832ea77-82b1-473f-97cd-5040b9881f1a"
      },
      "id": "N6hUIyj207X1",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,472\n",
            "              ReLU-2         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-3           [-1, 64, 56, 56]               0\n",
            "            Conv2d-4          [-1, 192, 56, 56]         110,784\n",
            "              ReLU-5          [-1, 192, 56, 56]               0\n",
            "         MaxPool2d-6          [-1, 192, 28, 28]               0\n",
            "            Conv2d-7          [-1, 128, 28, 28]          24,704\n",
            "              ReLU-8          [-1, 128, 28, 28]               0\n",
            "            Conv2d-9          [-1, 256, 28, 28]         295,168\n",
            "             ReLU-10          [-1, 256, 28, 28]               0\n",
            "        MaxPool2d-11          [-1, 256, 14, 14]               0\n",
            "           Conv2d-12          [-1, 256, 14, 14]         590,080\n",
            "             ReLU-13          [-1, 256, 14, 14]               0\n",
            "           Conv2d-14          [-1, 512, 14, 14]       1,180,160\n",
            "             ReLU-15          [-1, 512, 14, 14]               0\n",
            "        MaxPool2d-16            [-1, 512, 7, 7]               0\n",
            "           Conv2d-17            [-1, 512, 7, 7]       2,359,808\n",
            "             ReLU-18            [-1, 512, 7, 7]               0\n",
            "           Conv2d-19           [-1, 1024, 7, 7]       4,719,616\n",
            "             ReLU-20           [-1, 1024, 7, 7]               0\n",
            "           Conv2d-21           [-1, 1024, 7, 7]       9,438,208\n",
            "             ReLU-22           [-1, 1024, 7, 7]               0\n",
            "           Conv2d-23           [-1, 1024, 7, 7]       9,438,208\n",
            "             ReLU-24           [-1, 1024, 7, 7]               0\n",
            "          Flatten-25                [-1, 50176]               0\n",
            "           Linear-26                 [-1, 4096]     205,524,992\n",
            "             ReLU-27                 [-1, 4096]               0\n",
            "          Dropout-28                 [-1, 4096]               0\n",
            "           Linear-29                  [-1, 539]       2,208,283\n",
            "================================================================\n",
            "Total params: 235,899,483\n",
            "Trainable params: 235,899,483\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 34.74\n",
            "Params size (MB): 899.89\n",
            "Estimated Total Size (MB): 935.20\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e620c5c0",
      "metadata": {
        "id": "e620c5c0"
      },
      "outputs": [],
      "source": [
        "\n",
        "class WiderFaceDataset(Dataset):\n",
        "    def __init__(self, img_dir, label_dir, S=7, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.transform = transform\n",
        "        self.S = S\n",
        "        self.img_files = [f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.img_files[idx]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        label_path = os.path.join(self.label_dir, os.path.splitext(img_name)[0] + \".txt\")\n",
        "\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        h, w = img.size\n",
        "        target = torch.zeros((self.S, self.S, 5 * 2 + 1))  # B=2, C=1\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, \"r\") as f:\n",
        "                for line in f:\n",
        "                    cls, x_center, y_center, width, height = map(float, line.strip().split())\n",
        "                    grid_x = min(int(x_center * self.S), self.S - 1)\n",
        "                    grid_y = min(int(y_center * self.S), self.S - 1)\n",
        "                    # Assign to first predictor (simplified)\n",
        "                    target[grid_y, grid_x, 0] = x_center * self.S - grid_x\n",
        "                    target[grid_y, grid_x, 1] = y_center * self.S - grid_y\n",
        "                    target[grid_y, grid_x, 2] = width * self.S\n",
        "                    target[grid_y, grid_x, 3] = height * self.S\n",
        "                    target[grid_y, grid_x, 4] = 1.0  # Confidence\n",
        "                    target[grid_y, grid_x, 10] = cls\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b23918f",
      "metadata": {
        "id": "6b23918f"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def yolo_loss(preds, targets, S=7, B=2, lambda_coord=5.0, lambda_noobj=0.5):\n",
        "    batch_size = preds.size(0)\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        pred = preds[i].view(S, S, B * 5 + 1)  # [S, S, B*5 + C]\n",
        "        target = targets[i].view(S, S, B * 5 + 1)\n",
        "\n",
        "        for j in range(S):\n",
        "            for k in range(S):\n",
        "                # Objectness score (confidence)\n",
        "                obj_mask = target[j, k, 4] > 0  # True if object exists\n",
        "                noobj_mask = ~obj_mask\n",
        "\n",
        "                if obj_mask:\n",
        "                    # Coordinate loss (x, y, w, h) for the best bounding box\n",
        "                    best_iou = 0\n",
        "                    best_box_idx = 0\n",
        "                    for b in range(B):\n",
        "                        box_pred = pred[j, k, 5 * b:5 * (b + 1)]\n",
        "                        x, y, w, h, conf = box_pred\n",
        "                        target_box = target[j, k, :5]\n",
        "                        iou = calculate_iou((x, y, w, h), (target_box[0], target_box[1], target_box[2], target_box[3]))\n",
        "                        if iou > best_iou:\n",
        "                            best_iou = iou\n",
        "                            best_box_idx = b\n",
        "\n",
        "                    best_pred = pred[j, k, 5 * best_box_idx:5 * (best_box_idx + 1)]\n",
        "                    target_box = target[j, k, :5]\n",
        "                    coord_loss = nn.MSELoss()(best_pred[:4], target_box[:4]) * lambda_coord\n",
        "                    conf_loss = nn.MSELoss()(best_pred[4], target_box[4])\n",
        "                    class_loss = nn.MSELoss()(pred[j, k, 10], target[j, k, 10])\n",
        "\n",
        "                    total_loss += coord_loss + conf_loss + class_loss\n",
        "                # No object loss\n",
        "                for b in range(B):\n",
        "                    noobj_conf = pred[j, k, 5 * b + 4]\n",
        "                    zero_tensor = torch.tensor(0.0, device=pred.device)\n",
        "                    total_loss += nn.MSELoss()(noobj_conf, zero_tensor) * lambda_noobj * noobj_mask.float()\n",
        "\n",
        "    return total_loss / batch_size\n",
        "\n",
        "def calculate_iou(box1, box2):\n",
        "    # box: (x_center, y_center, width, height)\n",
        "    x1, y1, w1, h1 = box1\n",
        "    x2, y2, w2, h2 = box2\n",
        "    w1_half = w1 / 2\n",
        "    h1_half = h1 / 2\n",
        "    w2_half = w2 / 2\n",
        "    h2_half = h2 / 2\n",
        "\n",
        "    x1_min = x1 - w1_half\n",
        "    y1_min = y1 - h1_half\n",
        "    x1_max = x1 + w1_half\n",
        "    y1_max = y1 + h1_half\n",
        "\n",
        "    x2_min = x2 - w2_half\n",
        "    y2_min = y2 - h2_half\n",
        "    x2_max = x2 + w2_half\n",
        "    y2_max = y2 + h2_half\n",
        "\n",
        "    inter_x_min = max(x1_min, x2_min)\n",
        "    inter_y_min = max(y1_min, y2_min)\n",
        "    inter_x_max = min(x1_max, x2_max)\n",
        "    inter_y_max = min(y1_max, y2_max)\n",
        "\n",
        "    inter_area = max(0, inter_x_max - inter_x_min) * max(0, inter_y_max - inter_y_min)\n",
        "    union_area = w1 * h1 + w2 * h2 - inter_area\n",
        "\n",
        "    return inter_area / union_area if union_area > 0 else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b97c062",
      "metadata": {
        "id": "6b97c062"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Hyperparameters\n",
        "S = 7\n",
        "B = 2\n",
        "C = 1\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "batch_size = 16\n",
        "image_size = 224\n",
        "\n",
        "# Transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Dataset and DataLoader\n",
        "train_img_dir =   \"/home/cse/Desktop/Arif_30/dataset/WiderFace/archive/WIDER Face Dataset For YOLOv12/WIDER Face Dataset For YOLOv12/train/images\"  # Adjust path\n",
        "train_label_dir = \"/home/cse/Desktop/Arif_30/dataset/WiderFace/archive/WIDER Face Dataset For YOLOv12/WIDER Face Dataset For YOLOv12/train/labels\"  # Adjust path\n",
        "val_img_dir =     \"/home/cse/Desktop/Arif_30/dataset/WiderFace/archive/WIDER Face Dataset For YOLOv12/WIDER Face Dataset For YOLOv12/val/images\"  # Adjust path\n",
        "val_label_dir =   \"/home/cse/Desktop/Arif_30/dataset/WiderFace/archive/WIDER Face Dataset For YOLOv12/WIDER Face Dataset For YOLOv12/val/labels\"  # Adjust paths\n",
        "\n",
        "train_dataset = WiderFaceDataset(train_img_dir, train_label_dir, S=S, transform=transform)\n",
        "val_dataset = WiderFaceDataset(val_img_dir, val_label_dir, S=S, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = YOLOv1(S=S, B=B, C=C).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Metric\n",
        "metric = MeanAveragePrecision().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97197e31",
      "metadata": {
        "id": "97197e31",
        "outputId": "d5eb1715-6436-4a6d-e9da-63a545ffacc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Step [0/805], Loss: 16.7158\n",
            "Epoch [1/10], Step [10/805], Loss: 9.6947\n",
            "Epoch [1/10], Step [20/805], Loss: 9.4927\n",
            "Epoch [1/10], Step [30/805], Loss: 6.8855\n",
            "Epoch [1/10], Step [40/805], Loss: 12.5985\n",
            "Epoch [1/10], Step [50/805], Loss: 11.7831\n",
            "Epoch [1/10], Step [60/805], Loss: 7.9501\n",
            "Epoch [1/10], Step [70/805], Loss: 10.0053\n",
            "Epoch [1/10], Step [80/805], Loss: 10.0564\n",
            "Epoch [1/10], Step [90/805], Loss: 8.4344\n",
            "Epoch [1/10], Step [100/805], Loss: 8.7301\n",
            "Epoch [1/10], Step [110/805], Loss: 8.1152\n",
            "Epoch [1/10], Step [120/805], Loss: 6.2247\n",
            "Epoch [1/10], Step [130/805], Loss: 9.7800\n",
            "Epoch [1/10], Step [140/805], Loss: 6.1475\n",
            "Epoch [1/10], Step [150/805], Loss: 6.5112\n",
            "Epoch [1/10], Step [160/805], Loss: 6.0331\n",
            "Epoch [1/10], Step [170/805], Loss: 9.4308\n",
            "Epoch [1/10], Step [180/805], Loss: 6.8759\n",
            "Epoch [1/10], Step [190/805], Loss: 8.6972\n",
            "Epoch [1/10], Step [200/805], Loss: 8.5525\n",
            "Epoch [1/10], Step [210/805], Loss: 8.0714\n",
            "Epoch [1/10], Step [220/805], Loss: 6.3439\n",
            "Epoch [1/10], Step [230/805], Loss: 10.3800\n",
            "Epoch [1/10], Step [240/805], Loss: 7.9868\n",
            "Epoch [1/10], Step [250/805], Loss: 6.9925\n",
            "Epoch [1/10], Step [260/805], Loss: 9.2631\n",
            "Epoch [1/10], Step [270/805], Loss: 5.6346\n",
            "Epoch [1/10], Step [280/805], Loss: 9.5895\n",
            "Epoch [1/10], Step [290/805], Loss: 6.4129\n",
            "Epoch [1/10], Step [300/805], Loss: 8.0936\n",
            "Epoch [1/10], Step [310/805], Loss: 7.5759\n",
            "Epoch [1/10], Step [320/805], Loss: 10.5960\n",
            "Epoch [1/10], Step [330/805], Loss: 5.0838\n",
            "Epoch [1/10], Step [340/805], Loss: 7.1984\n",
            "Epoch [1/10], Step [350/805], Loss: 11.4269\n",
            "Epoch [1/10], Step [360/805], Loss: 9.0223\n",
            "Epoch [1/10], Step [370/805], Loss: 11.6521\n",
            "Epoch [1/10], Step [380/805], Loss: 7.7914\n",
            "Epoch [1/10], Step [390/805], Loss: 7.7086\n",
            "Epoch [1/10], Step [400/805], Loss: 6.5718\n",
            "Epoch [1/10], Step [410/805], Loss: 9.0698\n",
            "Epoch [1/10], Step [420/805], Loss: 12.1174\n",
            "Epoch [1/10], Step [430/805], Loss: 13.7089\n",
            "Epoch [1/10], Step [440/805], Loss: 11.6495\n",
            "Epoch [1/10], Step [450/805], Loss: 4.0152\n",
            "Epoch [1/10], Step [460/805], Loss: 8.8484\n",
            "Epoch [1/10], Step [470/805], Loss: 9.7140\n",
            "Epoch [1/10], Step [480/805], Loss: 9.8641\n",
            "Epoch [1/10], Step [490/805], Loss: 7.5331\n",
            "Epoch [1/10], Step [500/805], Loss: 6.0804\n",
            "Epoch [1/10], Step [510/805], Loss: 9.9285\n",
            "Epoch [1/10], Step [520/805], Loss: 10.0740\n",
            "Epoch [1/10], Step [530/805], Loss: 13.3255\n",
            "Epoch [1/10], Step [540/805], Loss: 8.5310\n",
            "Epoch [1/10], Step [550/805], Loss: 12.7667\n",
            "Epoch [1/10], Step [560/805], Loss: 7.4726\n",
            "Epoch [1/10], Step [570/805], Loss: 8.1435\n",
            "Epoch [1/10], Step [580/805], Loss: 10.9891\n",
            "Epoch [1/10], Step [590/805], Loss: 4.8563\n",
            "Epoch [1/10], Step [600/805], Loss: 8.4781\n",
            "Epoch [1/10], Step [610/805], Loss: 7.6531\n",
            "Epoch [1/10], Step [620/805], Loss: 3.9304\n",
            "Epoch [1/10], Step [630/805], Loss: 5.8176\n",
            "Epoch [1/10], Step [640/805], Loss: 8.5875\n",
            "Epoch [1/10], Step [650/805], Loss: 4.6951\n",
            "Epoch [1/10], Step [660/805], Loss: 10.5145\n",
            "Epoch [1/10], Step [670/805], Loss: 7.8832\n",
            "Epoch [1/10], Step [680/805], Loss: 5.3917\n",
            "Epoch [1/10], Step [690/805], Loss: 6.0532\n",
            "Epoch [1/10], Step [700/805], Loss: 16.8168\n",
            "Epoch [1/10], Step [710/805], Loss: 7.5580\n",
            "Epoch [1/10], Step [720/805], Loss: 5.4280\n",
            "Epoch [1/10], Step [730/805], Loss: 13.7673\n",
            "Epoch [1/10], Step [740/805], Loss: 11.6482\n",
            "Epoch [1/10], Step [750/805], Loss: 12.2803\n",
            "Epoch [1/10], Step [760/805], Loss: 7.1016\n",
            "Epoch [1/10], Step [770/805], Loss: 4.6598\n",
            "Epoch [1/10], Step [780/805], Loss: 9.0399\n",
            "Epoch [1/10], Step [790/805], Loss: 7.5928\n",
            "Epoch [1/10], Step [800/805], Loss: 6.3019\n",
            "Epoch [1/10], Average Loss: 8.7184\n",
            "Epoch [2/10], Step [0/805], Loss: 8.5634\n",
            "Epoch [2/10], Step [10/805], Loss: 10.0338\n",
            "Epoch [2/10], Step [20/805], Loss: 4.9064\n",
            "Epoch [2/10], Step [30/805], Loss: 7.5679\n",
            "Epoch [2/10], Step [40/805], Loss: 7.2530\n",
            "Epoch [2/10], Step [50/805], Loss: 10.3157\n",
            "Epoch [2/10], Step [60/805], Loss: 11.6180\n",
            "Epoch [2/10], Step [70/805], Loss: 7.4797\n",
            "Epoch [2/10], Step [80/805], Loss: 7.2359\n",
            "Epoch [2/10], Step [90/805], Loss: 12.7243\n",
            "Epoch [2/10], Step [100/805], Loss: 9.1753\n",
            "Epoch [2/10], Step [110/805], Loss: 6.7855\n",
            "Epoch [2/10], Step [120/805], Loss: 9.5111\n",
            "Epoch [2/10], Step [130/805], Loss: 5.7263\n",
            "Epoch [2/10], Step [140/805], Loss: 16.3052\n",
            "Epoch [2/10], Step [150/805], Loss: 11.1099\n",
            "Epoch [2/10], Step [160/805], Loss: 6.7797\n",
            "Epoch [2/10], Step [170/805], Loss: 14.5922\n",
            "Epoch [2/10], Step [180/805], Loss: 10.9154\n",
            "Epoch [2/10], Step [190/805], Loss: 11.8622\n",
            "Epoch [2/10], Step [200/805], Loss: 10.5663\n",
            "Epoch [2/10], Step [210/805], Loss: 12.6772\n",
            "Epoch [2/10], Step [220/805], Loss: 7.7398\n",
            "Epoch [2/10], Step [230/805], Loss: 10.7112\n",
            "Epoch [2/10], Step [240/805], Loss: 4.4990\n",
            "Epoch [2/10], Step [250/805], Loss: 7.7284\n",
            "Epoch [2/10], Step [260/805], Loss: 12.1902\n",
            "Epoch [2/10], Step [270/805], Loss: 6.1380\n",
            "Epoch [2/10], Step [280/805], Loss: 7.2157\n",
            "Epoch [2/10], Step [290/805], Loss: 15.7545\n",
            "Epoch [2/10], Step [300/805], Loss: 8.3470\n",
            "Epoch [2/10], Step [310/805], Loss: 10.5717\n",
            "Epoch [2/10], Step [320/805], Loss: 6.1948\n",
            "Epoch [2/10], Step [330/805], Loss: 11.5931\n",
            "Epoch [2/10], Step [340/805], Loss: 6.1706\n",
            "Epoch [2/10], Step [350/805], Loss: 9.5331\n",
            "Epoch [2/10], Step [360/805], Loss: 5.7211\n",
            "Epoch [2/10], Step [370/805], Loss: 8.7179\n",
            "Epoch [2/10], Step [380/805], Loss: 7.6777\n",
            "Epoch [2/10], Step [390/805], Loss: 8.0647\n",
            "Epoch [2/10], Step [400/805], Loss: 10.7515\n",
            "Epoch [2/10], Step [410/805], Loss: 5.1483\n",
            "Epoch [2/10], Step [420/805], Loss: 10.0246\n",
            "Epoch [2/10], Step [430/805], Loss: 5.1514\n",
            "Epoch [2/10], Step [440/805], Loss: 5.2744\n",
            "Epoch [2/10], Step [450/805], Loss: 7.7767\n",
            "Epoch [2/10], Step [460/805], Loss: 6.3294\n",
            "Epoch [2/10], Step [470/805], Loss: 5.9071\n",
            "Epoch [2/10], Step [480/805], Loss: 8.9391\n",
            "Epoch [2/10], Step [490/805], Loss: 6.2060\n",
            "Epoch [2/10], Step [500/805], Loss: 10.0652\n",
            "Epoch [2/10], Step [510/805], Loss: 6.3693\n",
            "Epoch [2/10], Step [520/805], Loss: 6.8884\n",
            "Epoch [2/10], Step [530/805], Loss: 7.5987\n",
            "Epoch [2/10], Step [540/805], Loss: 12.8728\n",
            "Epoch [2/10], Step [550/805], Loss: 6.0774\n",
            "Epoch [2/10], Step [560/805], Loss: 7.7391\n",
            "Epoch [2/10], Step [570/805], Loss: 10.0443\n",
            "Epoch [2/10], Step [580/805], Loss: 8.3878\n",
            "Epoch [2/10], Step [590/805], Loss: 14.5822\n",
            "Epoch [2/10], Step [600/805], Loss: 6.3974\n",
            "Epoch [2/10], Step [610/805], Loss: 8.5394\n",
            "Epoch [2/10], Step [620/805], Loss: 10.0215\n",
            "Epoch [2/10], Step [630/805], Loss: 6.6814\n",
            "Epoch [2/10], Step [640/805], Loss: 7.5644\n",
            "Epoch [2/10], Step [650/805], Loss: 8.1549\n",
            "Epoch [2/10], Step [660/805], Loss: 12.3697\n",
            "Epoch [2/10], Step [670/805], Loss: 8.9871\n",
            "Epoch [2/10], Step [680/805], Loss: 6.5905\n",
            "Epoch [2/10], Step [690/805], Loss: 7.3084\n",
            "Epoch [2/10], Step [700/805], Loss: 12.4702\n",
            "Epoch [2/10], Step [710/805], Loss: 4.9765\n",
            "Epoch [2/10], Step [720/805], Loss: 4.6610\n",
            "Epoch [2/10], Step [730/805], Loss: 5.3313\n",
            "Epoch [2/10], Step [740/805], Loss: 8.9270\n",
            "Epoch [2/10], Step [750/805], Loss: 4.5791\n",
            "Epoch [2/10], Step [760/805], Loss: 7.2667\n",
            "Epoch [2/10], Step [770/805], Loss: 9.0934\n",
            "Epoch [2/10], Step [780/805], Loss: 6.0843\n",
            "Epoch [2/10], Step [790/805], Loss: 8.3352\n",
            "Epoch [2/10], Step [800/805], Loss: 4.8920\n",
            "Epoch [2/10], Average Loss: 8.5619\n",
            "Epoch [3/10], Step [0/805], Loss: 11.5378\n",
            "Epoch [3/10], Step [10/805], Loss: 8.7180\n",
            "Epoch [3/10], Step [20/805], Loss: 9.3047\n",
            "Epoch [3/10], Step [30/805], Loss: 6.7150\n",
            "Epoch [3/10], Step [40/805], Loss: 7.4443\n",
            "Epoch [3/10], Step [50/805], Loss: 10.7933\n",
            "Epoch [3/10], Step [60/805], Loss: 6.8992\n",
            "Epoch [3/10], Step [70/805], Loss: 8.3689\n",
            "Epoch [3/10], Step [80/805], Loss: 6.3269\n",
            "Epoch [3/10], Step [90/805], Loss: 7.1980\n",
            "Epoch [3/10], Step [100/805], Loss: 7.0295\n",
            "Epoch [3/10], Step [110/805], Loss: 8.6099\n",
            "Epoch [3/10], Step [120/805], Loss: 9.4372\n",
            "Epoch [3/10], Step [130/805], Loss: 8.7745\n",
            "Epoch [3/10], Step [140/805], Loss: 7.2283\n",
            "Epoch [3/10], Step [150/805], Loss: 9.0716\n",
            "Epoch [3/10], Step [160/805], Loss: 7.2187\n",
            "Epoch [3/10], Step [170/805], Loss: 8.5079\n",
            "Epoch [3/10], Step [180/805], Loss: 11.1755\n",
            "Epoch [3/10], Step [190/805], Loss: 8.2195\n",
            "Epoch [3/10], Step [200/805], Loss: 7.3150\n",
            "Epoch [3/10], Step [210/805], Loss: 11.8633\n",
            "Epoch [3/10], Step [220/805], Loss: 7.3084\n",
            "Epoch [3/10], Step [230/805], Loss: 4.8608\n",
            "Epoch [3/10], Step [240/805], Loss: 10.6865\n",
            "Epoch [3/10], Step [250/805], Loss: 4.8675\n",
            "Epoch [3/10], Step [260/805], Loss: 4.7492\n",
            "Epoch [3/10], Step [270/805], Loss: 8.7932\n",
            "Epoch [3/10], Step [280/805], Loss: 10.8820\n",
            "Epoch [3/10], Step [290/805], Loss: 18.9270\n",
            "Epoch [3/10], Step [300/805], Loss: 12.9131\n",
            "Epoch [3/10], Step [310/805], Loss: 9.2516\n",
            "Epoch [3/10], Step [320/805], Loss: 7.1483\n",
            "Epoch [3/10], Step [330/805], Loss: 10.2886\n",
            "Epoch [3/10], Step [340/805], Loss: 6.2230\n",
            "Epoch [3/10], Step [350/805], Loss: 7.5296\n",
            "Epoch [3/10], Step [360/805], Loss: 10.4797\n",
            "Epoch [3/10], Step [370/805], Loss: 9.2477\n",
            "Epoch [3/10], Step [380/805], Loss: 14.2334\n",
            "Epoch [3/10], Step [390/805], Loss: 8.8718\n",
            "Epoch [3/10], Step [400/805], Loss: 5.8778\n",
            "Epoch [3/10], Step [410/805], Loss: 12.7402\n",
            "Epoch [3/10], Step [420/805], Loss: 9.8864\n",
            "Epoch [3/10], Step [430/805], Loss: 7.3859\n",
            "Epoch [3/10], Step [440/805], Loss: 12.0336\n",
            "Epoch [3/10], Step [450/805], Loss: 7.1250\n",
            "Epoch [3/10], Step [460/805], Loss: 4.4726\n",
            "Epoch [3/10], Step [470/805], Loss: 13.1672\n",
            "Epoch [3/10], Step [480/805], Loss: 8.5972\n",
            "Epoch [3/10], Step [490/805], Loss: 6.9067\n",
            "Epoch [3/10], Step [500/805], Loss: 12.3458\n",
            "Epoch [3/10], Step [510/805], Loss: 8.4235\n",
            "Epoch [3/10], Step [520/805], Loss: 8.3468\n",
            "Epoch [3/10], Step [530/805], Loss: 7.9400\n",
            "Epoch [3/10], Step [540/805], Loss: 5.6056\n",
            "Epoch [3/10], Step [550/805], Loss: 7.5646\n",
            "Epoch [3/10], Step [560/805], Loss: 8.3929\n",
            "Epoch [3/10], Step [570/805], Loss: 10.7265\n",
            "Epoch [3/10], Step [580/805], Loss: 5.8203\n",
            "Epoch [3/10], Step [590/805], Loss: 10.7034\n",
            "Epoch [3/10], Step [600/805], Loss: 6.2746\n",
            "Epoch [3/10], Step [610/805], Loss: 6.4831\n",
            "Epoch [3/10], Step [620/805], Loss: 5.9262\n",
            "Epoch [3/10], Step [630/805], Loss: 7.2993\n",
            "Epoch [3/10], Step [640/805], Loss: 9.8146\n",
            "Epoch [3/10], Step [650/805], Loss: 7.2233\n",
            "Epoch [3/10], Step [660/805], Loss: 12.3447\n",
            "Epoch [3/10], Step [670/805], Loss: 6.6403\n",
            "Epoch [3/10], Step [680/805], Loss: 8.0150\n",
            "Epoch [3/10], Step [690/805], Loss: 5.9991\n",
            "Epoch [3/10], Step [700/805], Loss: 7.4126\n",
            "Epoch [3/10], Step [710/805], Loss: 6.1476\n",
            "Epoch [3/10], Step [720/805], Loss: 11.1640\n",
            "Epoch [3/10], Step [730/805], Loss: 11.0721\n",
            "Epoch [3/10], Step [740/805], Loss: 6.0801\n",
            "Epoch [3/10], Step [750/805], Loss: 8.2856\n",
            "Epoch [3/10], Step [760/805], Loss: 5.1892\n",
            "Epoch [3/10], Step [770/805], Loss: 7.2317\n",
            "Epoch [3/10], Step [780/805], Loss: 11.3414\n",
            "Epoch [3/10], Step [790/805], Loss: 8.4106\n",
            "Epoch [3/10], Step [800/805], Loss: 8.8833\n",
            "Epoch [3/10], Average Loss: 8.5144\n",
            "Epoch [4/10], Step [0/805], Loss: 19.1296\n",
            "Epoch [4/10], Step [10/805], Loss: 10.3547\n",
            "Epoch [4/10], Step [20/805], Loss: 8.6283\n",
            "Epoch [4/10], Step [30/805], Loss: 13.4294\n",
            "Epoch [4/10], Step [40/805], Loss: 14.3819\n",
            "Epoch [4/10], Step [50/805], Loss: 531.4472\n",
            "Epoch [4/10], Step [60/805], Loss: 13646.8574\n",
            "Epoch [4/10], Step [70/805], Loss: 28831.2852\n",
            "Epoch [4/10], Step [80/805], Loss: 1690.7628\n",
            "Epoch [4/10], Step [90/805], Loss: 55846.5625\n",
            "Epoch [4/10], Step [100/805], Loss: 9109.7793\n",
            "Epoch [4/10], Step [110/805], Loss: 10.4558\n",
            "Epoch [4/10], Step [120/805], Loss: 16.0032\n",
            "Epoch [4/10], Step [130/805], Loss: 16.1992\n",
            "Epoch [4/10], Step [140/805], Loss: 11.5455\n",
            "Epoch [4/10], Step [150/805], Loss: 13.7255\n",
            "Epoch [4/10], Step [160/805], Loss: 11.9001\n",
            "Epoch [4/10], Step [170/805], Loss: 15.5233\n",
            "Epoch [4/10], Step [180/805], Loss: 15.8649\n",
            "Epoch [4/10], Step [190/805], Loss: 19.3941\n",
            "Epoch [4/10], Step [200/805], Loss: 13.8063\n",
            "Epoch [4/10], Step [210/805], Loss: 9.7879\n",
            "Epoch [4/10], Step [220/805], Loss: 20.0142\n",
            "Epoch [4/10], Step [230/805], Loss: 16.7428\n",
            "Epoch [4/10], Step [240/805], Loss: 17.3177\n",
            "Epoch [4/10], Step [250/805], Loss: 18.1130\n",
            "Epoch [4/10], Step [260/805], Loss: 12.2616\n",
            "Epoch [4/10], Step [270/805], Loss: 11.0112\n",
            "Epoch [4/10], Step [280/805], Loss: 14.1707\n",
            "Epoch [4/10], Step [290/805], Loss: 10.1070\n",
            "Epoch [4/10], Step [300/805], Loss: 16.4711\n",
            "Epoch [4/10], Step [310/805], Loss: 12.8477\n",
            "Epoch [4/10], Step [320/805], Loss: 18.6164\n",
            "Epoch [4/10], Step [330/805], Loss: 16.3720\n",
            "Epoch [4/10], Step [340/805], Loss: 12.9199\n",
            "Epoch [4/10], Step [350/805], Loss: 12.7701\n",
            "Epoch [4/10], Step [360/805], Loss: 16.3399\n",
            "Epoch [4/10], Step [370/805], Loss: 11.2656\n",
            "Epoch [4/10], Step [380/805], Loss: 11.2246\n",
            "Epoch [4/10], Step [390/805], Loss: 11.9694\n",
            "Epoch [4/10], Step [400/805], Loss: 12.8755\n",
            "Epoch [4/10], Step [410/805], Loss: 13.8860\n",
            "Epoch [4/10], Step [420/805], Loss: 13.5711\n",
            "Epoch [4/10], Step [430/805], Loss: 10.1480\n",
            "Epoch [4/10], Step [440/805], Loss: 9.1258\n",
            "Epoch [4/10], Step [450/805], Loss: 13.9980\n",
            "Epoch [4/10], Step [460/805], Loss: 14.9198\n",
            "Epoch [4/10], Step [470/805], Loss: 10.7991\n",
            "Epoch [4/10], Step [480/805], Loss: 11.3474\n",
            "Epoch [4/10], Step [490/805], Loss: 12.3058\n",
            "Epoch [4/10], Step [500/805], Loss: 15.2104\n",
            "Epoch [4/10], Step [510/805], Loss: 12.8313\n",
            "Epoch [4/10], Step [520/805], Loss: 17.9256\n",
            "Epoch [4/10], Step [530/805], Loss: 9.8482\n",
            "Epoch [4/10], Step [540/805], Loss: 11.8137\n",
            "Epoch [4/10], Step [550/805], Loss: 13.1061\n",
            "Epoch [4/10], Step [560/805], Loss: 18.6366\n",
            "Epoch [4/10], Step [570/805], Loss: 19.0474\n",
            "Epoch [4/10], Step [580/805], Loss: 17.6820\n",
            "Epoch [4/10], Step [590/805], Loss: 10.9102\n",
            "Epoch [4/10], Step [600/805], Loss: 13.2321\n",
            "Epoch [4/10], Step [610/805], Loss: 14.5906\n",
            "Epoch [4/10], Step [620/805], Loss: 9.9105\n",
            "Epoch [4/10], Step [630/805], Loss: 9.8341\n",
            "Epoch [4/10], Step [640/805], Loss: 15.0279\n",
            "Epoch [4/10], Step [650/805], Loss: 14.0135\n",
            "Epoch [4/10], Step [660/805], Loss: 15.4125\n",
            "Epoch [4/10], Step [670/805], Loss: 12.5042\n",
            "Epoch [4/10], Step [680/805], Loss: 11.0383\n",
            "Epoch [4/10], Step [690/805], Loss: 9.9632\n",
            "Epoch [4/10], Step [700/805], Loss: 10.1783\n",
            "Epoch [4/10], Step [710/805], Loss: 14.6686\n",
            "Epoch [4/10], Step [720/805], Loss: 11.3727\n",
            "Epoch [4/10], Step [730/805], Loss: 9.1849\n",
            "Epoch [4/10], Step [740/805], Loss: 12.3858\n",
            "Epoch [4/10], Step [750/805], Loss: 20.4565\n",
            "Epoch [4/10], Step [760/805], Loss: 16.2087\n",
            "Epoch [4/10], Step [770/805], Loss: 12.0630\n",
            "Epoch [4/10], Step [780/805], Loss: 13.5430\n",
            "Epoch [4/10], Step [790/805], Loss: 14.5375\n",
            "Epoch [4/10], Step [800/805], Loss: 9.3752\n",
            "Epoch [4/10], Average Loss: 37167.9758\n",
            "Epoch [5/10], Step [0/805], Loss: 9.3044\n",
            "Epoch [5/10], Step [10/805], Loss: 13.3995\n",
            "Epoch [5/10], Step [20/805], Loss: 14.8669\n",
            "Epoch [5/10], Step [30/805], Loss: 10.7903\n",
            "Epoch [5/10], Step [40/805], Loss: 14.5394\n",
            "Epoch [5/10], Step [50/805], Loss: 9.2201\n",
            "Epoch [5/10], Step [60/805], Loss: 14.6268\n",
            "Epoch [5/10], Step [70/805], Loss: 14.3945\n",
            "Epoch [5/10], Step [80/805], Loss: 8.6299\n",
            "Epoch [5/10], Step [90/805], Loss: 11.1384\n",
            "Epoch [5/10], Step [100/805], Loss: 15.8461\n",
            "Epoch [5/10], Step [110/805], Loss: 14.3504\n",
            "Epoch [5/10], Step [120/805], Loss: 10.8282\n",
            "Epoch [5/10], Step [130/805], Loss: 12.2342\n",
            "Epoch [5/10], Step [140/805], Loss: 13.6468\n",
            "Epoch [5/10], Step [150/805], Loss: 13.3119\n",
            "Epoch [5/10], Step [160/805], Loss: 13.5262\n",
            "Epoch [5/10], Step [170/805], Loss: 15.4608\n",
            "Epoch [5/10], Step [180/805], Loss: 15.7231\n",
            "Epoch [5/10], Step [190/805], Loss: 11.1751\n",
            "Epoch [5/10], Step [200/805], Loss: 16.3303\n",
            "Epoch [5/10], Step [210/805], Loss: 16.5774\n",
            "Epoch [5/10], Step [220/805], Loss: 9.0173\n",
            "Epoch [5/10], Step [230/805], Loss: 15.2952\n",
            "Epoch [5/10], Step [240/805], Loss: 8.8895\n",
            "Epoch [5/10], Step [250/805], Loss: 10.6572\n",
            "Epoch [5/10], Step [260/805], Loss: 8.9683\n",
            "Epoch [5/10], Step [270/805], Loss: 18.2185\n",
            "Epoch [5/10], Step [280/805], Loss: 8.9141\n",
            "Epoch [5/10], Step [290/805], Loss: 10.4852\n",
            "Epoch [5/10], Step [300/805], Loss: 12.7060\n",
            "Epoch [5/10], Step [310/805], Loss: 15.0992\n",
            "Epoch [5/10], Step [320/805], Loss: 11.7501\n",
            "Epoch [5/10], Step [330/805], Loss: 7.9227\n",
            "Epoch [5/10], Step [340/805], Loss: 13.8067\n",
            "Epoch [5/10], Step [350/805], Loss: 10.0778\n",
            "Epoch [5/10], Step [360/805], Loss: 8.8645\n",
            "Epoch [5/10], Step [370/805], Loss: 8.1049\n",
            "Epoch [5/10], Step [380/805], Loss: 7.5899\n",
            "Epoch [5/10], Step [390/805], Loss: 9.4754\n",
            "Epoch [5/10], Step [400/805], Loss: 10.4006\n",
            "Epoch [5/10], Step [410/805], Loss: 10.5755\n",
            "Epoch [5/10], Step [420/805], Loss: 16.8576\n",
            "Epoch [5/10], Step [430/805], Loss: 10.0728\n",
            "Epoch [5/10], Step [440/805], Loss: 7.0517\n",
            "Epoch [5/10], Step [450/805], Loss: 12.3183\n",
            "Epoch [5/10], Step [460/805], Loss: 15.7829\n",
            "Epoch [5/10], Step [470/805], Loss: 12.0258\n",
            "Epoch [5/10], Step [480/805], Loss: 10.4461\n",
            "Epoch [5/10], Step [490/805], Loss: 11.7453\n",
            "Epoch [5/10], Step [500/805], Loss: 11.9105\n",
            "Epoch [5/10], Step [510/805], Loss: 6.0968\n",
            "Epoch [5/10], Step [520/805], Loss: 18.1554\n",
            "Epoch [5/10], Step [530/805], Loss: 14.6675\n",
            "Epoch [5/10], Step [540/805], Loss: 17.1237\n",
            "Epoch [5/10], Step [550/805], Loss: 7.1659\n",
            "Epoch [5/10], Step [560/805], Loss: 11.6165\n",
            "Epoch [5/10], Step [570/805], Loss: 8.4307\n",
            "Epoch [5/10], Step [580/805], Loss: 17.3668\n",
            "Epoch [5/10], Step [590/805], Loss: 12.6876\n",
            "Epoch [5/10], Step [600/805], Loss: 15.6922\n",
            "Epoch [5/10], Step [610/805], Loss: 15.9111\n",
            "Epoch [5/10], Step [620/805], Loss: 15.1670\n",
            "Epoch [5/10], Step [630/805], Loss: 10.5863\n",
            "Epoch [5/10], Step [640/805], Loss: 9.0086\n",
            "Epoch [5/10], Step [650/805], Loss: 9.6617\n",
            "Epoch [5/10], Step [660/805], Loss: 14.6531\n",
            "Epoch [5/10], Step [670/805], Loss: 14.3223\n",
            "Epoch [5/10], Step [680/805], Loss: 14.0448\n",
            "Epoch [5/10], Step [690/805], Loss: 10.5663\n",
            "Epoch [5/10], Step [700/805], Loss: 9.1815\n",
            "Epoch [5/10], Step [710/805], Loss: 7.9505\n",
            "Epoch [5/10], Step [720/805], Loss: 18.1171\n",
            "Epoch [5/10], Step [730/805], Loss: 15.1805\n",
            "Epoch [5/10], Step [740/805], Loss: 11.8187\n",
            "Epoch [5/10], Step [750/805], Loss: 16.8593\n",
            "Epoch [5/10], Step [760/805], Loss: 12.2960\n",
            "Epoch [5/10], Step [770/805], Loss: 9.6865\n",
            "Epoch [5/10], Step [780/805], Loss: 8.6114\n",
            "Epoch [5/10], Step [790/805], Loss: 10.1652\n",
            "Epoch [5/10], Step [800/805], Loss: 10.8106\n",
            "Epoch [5/10], Average Loss: 11.9440\n",
            "Epoch [6/10], Step [0/805], Loss: 7.1701\n",
            "Epoch [6/10], Step [10/805], Loss: 12.9148\n",
            "Epoch [6/10], Step [20/805], Loss: 9.6645\n",
            "Epoch [6/10], Step [30/805], Loss: 8.7850\n",
            "Epoch [6/10], Step [40/805], Loss: 15.5333\n",
            "Epoch [6/10], Step [50/805], Loss: 14.5205\n",
            "Epoch [6/10], Step [60/805], Loss: 6.9846\n",
            "Epoch [6/10], Step [70/805], Loss: 7.1585\n",
            "Epoch [6/10], Step [80/805], Loss: 8.8023\n",
            "Epoch [6/10], Step [90/805], Loss: 8.8161\n",
            "Epoch [6/10], Step [100/805], Loss: 12.0677\n",
            "Epoch [6/10], Step [110/805], Loss: 11.1922\n",
            "Epoch [6/10], Step [120/805], Loss: 10.6794\n",
            "Epoch [6/10], Step [130/805], Loss: 16.7902\n",
            "Epoch [6/10], Step [140/805], Loss: 14.1438\n",
            "Epoch [6/10], Step [150/805], Loss: 11.5465\n",
            "Epoch [6/10], Step [160/805], Loss: 12.7186\n",
            "Epoch [6/10], Step [170/805], Loss: 11.2785\n",
            "Epoch [6/10], Step [180/805], Loss: 14.9822\n",
            "Epoch [6/10], Step [190/805], Loss: 13.4262\n",
            "Epoch [6/10], Step [200/805], Loss: 13.5635\n",
            "Epoch [6/10], Step [210/805], Loss: 14.2773\n",
            "Epoch [6/10], Step [220/805], Loss: 10.4993\n",
            "Epoch [6/10], Step [230/805], Loss: 10.3063\n",
            "Epoch [6/10], Step [240/805], Loss: 10.2270\n",
            "Epoch [6/10], Step [250/805], Loss: 8.7136\n",
            "Epoch [6/10], Step [260/805], Loss: 7.6682\n",
            "Epoch [6/10], Step [270/805], Loss: 11.2812\n",
            "Epoch [6/10], Step [280/805], Loss: 15.4382\n",
            "Epoch [6/10], Step [290/805], Loss: 9.7322\n",
            "Epoch [6/10], Step [300/805], Loss: 8.4785\n",
            "Epoch [6/10], Step [310/805], Loss: 11.3092\n",
            "Epoch [6/10], Step [320/805], Loss: 11.4141\n",
            "Epoch [6/10], Step [330/805], Loss: 9.4812\n",
            "Epoch [6/10], Step [340/805], Loss: 14.3818\n",
            "Epoch [6/10], Step [350/805], Loss: 8.5515\n",
            "Epoch [6/10], Step [360/805], Loss: 10.1088\n",
            "Epoch [6/10], Step [370/805], Loss: 12.6877\n",
            "Epoch [6/10], Step [380/805], Loss: 6.0453\n",
            "Epoch [6/10], Step [390/805], Loss: 14.4591\n",
            "Epoch [6/10], Step [400/805], Loss: 13.4466\n",
            "Epoch [6/10], Step [410/805], Loss: 19.5209\n",
            "Epoch [6/10], Step [420/805], Loss: 11.2452\n",
            "Epoch [6/10], Step [430/805], Loss: 14.6308\n",
            "Epoch [6/10], Step [440/805], Loss: 16.7157\n",
            "Epoch [6/10], Step [450/805], Loss: 10.2849\n",
            "Epoch [6/10], Step [460/805], Loss: 12.5964\n",
            "Epoch [6/10], Step [470/805], Loss: 14.2012\n",
            "Epoch [6/10], Step [480/805], Loss: 17.9468\n",
            "Epoch [6/10], Step [490/805], Loss: 11.2662\n",
            "Epoch [6/10], Step [500/805], Loss: 8.6524\n",
            "Epoch [6/10], Step [510/805], Loss: 6.0681\n",
            "Epoch [6/10], Step [520/805], Loss: 10.1703\n",
            "Epoch [6/10], Step [530/805], Loss: 4.3288\n",
            "Epoch [6/10], Step [540/805], Loss: 9.6380\n",
            "Epoch [6/10], Step [550/805], Loss: 9.2369\n",
            "Epoch [6/10], Step [560/805], Loss: 7.1451\n",
            "Epoch [6/10], Step [570/805], Loss: 10.9856\n",
            "Epoch [6/10], Step [580/805], Loss: 10.9114\n",
            "Epoch [6/10], Step [590/805], Loss: 11.9696\n",
            "Epoch [6/10], Step [600/805], Loss: 10.6528\n",
            "Epoch [6/10], Step [610/805], Loss: 11.3983\n",
            "Epoch [6/10], Step [620/805], Loss: 9.4651\n",
            "Epoch [6/10], Step [630/805], Loss: 13.0156\n",
            "Epoch [6/10], Step [640/805], Loss: 13.4056\n",
            "Epoch [6/10], Step [650/805], Loss: 14.4057\n",
            "Epoch [6/10], Step [660/805], Loss: 9.4397\n",
            "Epoch [6/10], Step [670/805], Loss: 8.4848\n",
            "Epoch [6/10], Step [680/805], Loss: 11.5421\n",
            "Epoch [6/10], Step [690/805], Loss: 10.9740\n",
            "Epoch [6/10], Step [700/805], Loss: 9.4705\n",
            "Epoch [6/10], Step [710/805], Loss: 14.1022\n",
            "Epoch [6/10], Step [720/805], Loss: 5.7049\n",
            "Epoch [6/10], Step [730/805], Loss: 14.3618\n",
            "Epoch [6/10], Step [740/805], Loss: 9.7294\n",
            "Epoch [6/10], Step [750/805], Loss: 11.4110\n",
            "Epoch [6/10], Step [760/805], Loss: 12.8775\n",
            "Epoch [6/10], Step [770/805], Loss: 7.6524\n",
            "Epoch [6/10], Step [780/805], Loss: 8.7249\n",
            "Epoch [6/10], Step [790/805], Loss: 10.5367\n",
            "Epoch [6/10], Step [800/805], Loss: 12.1827\n",
            "Epoch [6/10], Average Loss: 10.7633\n",
            "Epoch [7/10], Step [0/805], Loss: 7.5083\n",
            "Epoch [7/10], Step [10/805], Loss: 7.7920\n",
            "Epoch [7/10], Step [20/805], Loss: 12.0373\n",
            "Epoch [7/10], Step [30/805], Loss: 6.9036\n",
            "Epoch [7/10], Step [40/805], Loss: 7.7379\n",
            "Epoch [7/10], Step [50/805], Loss: 10.2834\n",
            "Epoch [7/10], Step [60/805], Loss: 11.8653\n",
            "Epoch [7/10], Step [70/805], Loss: 7.9129\n",
            "Epoch [7/10], Step [80/805], Loss: 15.6264\n",
            "Epoch [7/10], Step [90/805], Loss: 11.3226\n",
            "Epoch [7/10], Step [100/805], Loss: 8.5815\n",
            "Epoch [7/10], Step [110/805], Loss: 11.3492\n",
            "Epoch [7/10], Step [120/805], Loss: 7.1402\n",
            "Epoch [7/10], Step [130/805], Loss: 10.9735\n",
            "Epoch [7/10], Step [140/805], Loss: 5.6396\n",
            "Epoch [7/10], Step [150/805], Loss: 6.8731\n",
            "Epoch [7/10], Step [160/805], Loss: 8.8453\n",
            "Epoch [7/10], Step [170/805], Loss: 10.8581\n",
            "Epoch [7/10], Step [180/805], Loss: 10.5341\n",
            "Epoch [7/10], Step [190/805], Loss: 10.8591\n",
            "Epoch [7/10], Step [200/805], Loss: 10.3967\n",
            "Epoch [7/10], Step [210/805], Loss: 12.0070\n",
            "Epoch [7/10], Step [220/805], Loss: 15.9092\n",
            "Epoch [7/10], Step [230/805], Loss: 10.6477\n",
            "Epoch [7/10], Step [240/805], Loss: 9.9015\n",
            "Epoch [7/10], Step [250/805], Loss: 9.0033\n",
            "Epoch [7/10], Step [260/805], Loss: 11.6825\n",
            "Epoch [7/10], Step [270/805], Loss: 9.6207\n",
            "Epoch [7/10], Step [280/805], Loss: 13.1483\n",
            "Epoch [7/10], Step [290/805], Loss: 14.0212\n",
            "Epoch [7/10], Step [300/805], Loss: 8.0763\n",
            "Epoch [7/10], Step [310/805], Loss: 7.1736\n",
            "Epoch [7/10], Step [320/805], Loss: 11.1462\n",
            "Epoch [7/10], Step [330/805], Loss: 8.6743\n",
            "Epoch [7/10], Step [340/805], Loss: 11.7795\n",
            "Epoch [7/10], Step [350/805], Loss: 14.9412\n",
            "Epoch [7/10], Step [360/805], Loss: 8.6065\n",
            "Epoch [7/10], Step [370/805], Loss: 9.6816\n",
            "Epoch [7/10], Step [380/805], Loss: 9.2633\n",
            "Epoch [7/10], Step [390/805], Loss: 9.2991\n",
            "Epoch [7/10], Step [400/805], Loss: 8.2834\n",
            "Epoch [7/10], Step [410/805], Loss: 10.7086\n",
            "Epoch [7/10], Step [420/805], Loss: 10.3598\n",
            "Epoch [7/10], Step [430/805], Loss: 15.5153\n",
            "Epoch [7/10], Step [440/805], Loss: 9.8927\n",
            "Epoch [7/10], Step [450/805], Loss: 10.9821\n",
            "Epoch [7/10], Step [460/805], Loss: 5.7423\n",
            "Epoch [7/10], Step [470/805], Loss: 13.8952\n",
            "Epoch [7/10], Step [480/805], Loss: 6.8809\n",
            "Epoch [7/10], Step [490/805], Loss: 9.3291\n",
            "Epoch [7/10], Step [500/805], Loss: 9.9740\n",
            "Epoch [7/10], Step [510/805], Loss: 8.5133\n",
            "Epoch [7/10], Step [520/805], Loss: 9.2985\n",
            "Epoch [7/10], Step [530/805], Loss: 6.9555\n",
            "Epoch [7/10], Step [540/805], Loss: 8.3568\n",
            "Epoch [7/10], Step [550/805], Loss: 8.1810\n",
            "Epoch [7/10], Step [560/805], Loss: 6.4406\n",
            "Epoch [7/10], Step [570/805], Loss: 13.0480\n",
            "Epoch [7/10], Step [580/805], Loss: 6.1552\n",
            "Epoch [7/10], Step [590/805], Loss: 10.2188\n",
            "Epoch [7/10], Step [600/805], Loss: 5.3635\n",
            "Epoch [7/10], Step [610/805], Loss: 11.9886\n",
            "Epoch [7/10], Step [620/805], Loss: 9.4976\n",
            "Epoch [7/10], Step [630/805], Loss: 10.5964\n",
            "Epoch [7/10], Step [640/805], Loss: 7.5569\n",
            "Epoch [7/10], Step [650/805], Loss: 9.8829\n",
            "Epoch [7/10], Step [660/805], Loss: 11.5130\n",
            "Epoch [7/10], Step [670/805], Loss: 8.2183\n",
            "Epoch [7/10], Step [680/805], Loss: 6.7351\n",
            "Epoch [7/10], Step [690/805], Loss: 17.2873\n",
            "Epoch [7/10], Step [700/805], Loss: 10.8183\n",
            "Epoch [7/10], Step [710/805], Loss: 9.2150\n",
            "Epoch [7/10], Step [720/805], Loss: 8.4648\n",
            "Epoch [7/10], Step [730/805], Loss: 10.7657\n",
            "Epoch [7/10], Step [740/805], Loss: 8.0511\n",
            "Epoch [7/10], Step [750/805], Loss: 12.1575\n",
            "Epoch [7/10], Step [760/805], Loss: 8.0810\n",
            "Epoch [7/10], Step [770/805], Loss: 11.6635\n",
            "Epoch [7/10], Step [780/805], Loss: 8.4400\n",
            "Epoch [7/10], Step [790/805], Loss: 7.5325\n",
            "Epoch [7/10], Step [800/805], Loss: 9.0930\n",
            "Epoch [7/10], Average Loss: 10.0159\n",
            "Epoch [8/10], Step [0/805], Loss: 7.8944\n",
            "Epoch [8/10], Step [10/805], Loss: 10.6269\n",
            "Epoch [8/10], Step [20/805], Loss: 11.0474\n",
            "Epoch [8/10], Step [30/805], Loss: 11.8905\n",
            "Epoch [8/10], Step [40/805], Loss: 16.4810\n",
            "Epoch [8/10], Step [50/805], Loss: 12.7454\n",
            "Epoch [8/10], Step [60/805], Loss: 13.1104\n",
            "Epoch [8/10], Step [70/805], Loss: 8.5100\n",
            "Epoch [8/10], Step [80/805], Loss: 16.8415\n",
            "Epoch [8/10], Step [90/805], Loss: 8.5177\n",
            "Epoch [8/10], Step [100/805], Loss: 8.9643\n",
            "Epoch [8/10], Step [110/805], Loss: 6.9144\n",
            "Epoch [8/10], Step [120/805], Loss: 10.8067\n",
            "Epoch [8/10], Step [130/805], Loss: 3.9560\n",
            "Epoch [8/10], Step [140/805], Loss: 9.8285\n",
            "Epoch [8/10], Step [150/805], Loss: 7.5652\n",
            "Epoch [8/10], Step [160/805], Loss: 7.2705\n",
            "Epoch [8/10], Step [170/805], Loss: 9.1927\n",
            "Epoch [8/10], Step [180/805], Loss: 10.9698\n",
            "Epoch [8/10], Step [190/805], Loss: 10.3450\n",
            "Epoch [8/10], Step [200/805], Loss: 4.4342\n",
            "Epoch [8/10], Step [210/805], Loss: 12.2469\n",
            "Epoch [8/10], Step [220/805], Loss: 8.6421\n",
            "Epoch [8/10], Step [230/805], Loss: 16.2199\n",
            "Epoch [8/10], Step [240/805], Loss: 9.4834\n",
            "Epoch [8/10], Step [250/805], Loss: 11.3331\n",
            "Epoch [8/10], Step [260/805], Loss: 15.7175\n",
            "Epoch [8/10], Step [270/805], Loss: 7.9047\n",
            "Epoch [8/10], Step [280/805], Loss: 6.0117\n",
            "Epoch [8/10], Step [290/805], Loss: 8.4010\n",
            "Epoch [8/10], Step [300/805], Loss: 7.2072\n",
            "Epoch [8/10], Step [310/805], Loss: 7.8554\n",
            "Epoch [8/10], Step [320/805], Loss: 8.0210\n",
            "Epoch [8/10], Step [330/805], Loss: 9.9320\n",
            "Epoch [8/10], Step [340/805], Loss: 12.5157\n",
            "Epoch [8/10], Step [350/805], Loss: 14.5726\n",
            "Epoch [8/10], Step [360/805], Loss: 10.4470\n",
            "Epoch [8/10], Step [370/805], Loss: 6.1861\n",
            "Epoch [8/10], Step [380/805], Loss: 6.7568\n",
            "Epoch [8/10], Step [390/805], Loss: 6.9455\n",
            "Epoch [8/10], Step [400/805], Loss: 10.1725\n",
            "Epoch [8/10], Step [410/805], Loss: 7.8789\n",
            "Epoch [8/10], Step [420/805], Loss: 13.1931\n",
            "Epoch [8/10], Step [430/805], Loss: 10.6730\n",
            "Epoch [8/10], Step [440/805], Loss: 6.1904\n",
            "Epoch [8/10], Step [450/805], Loss: 11.2707\n",
            "Epoch [8/10], Step [460/805], Loss: 10.1837\n",
            "Epoch [8/10], Step [470/805], Loss: 6.7193\n",
            "Epoch [8/10], Step [480/805], Loss: 10.0309\n",
            "Epoch [8/10], Step [490/805], Loss: 6.2706\n",
            "Epoch [8/10], Step [500/805], Loss: 13.4971\n",
            "Epoch [8/10], Step [510/805], Loss: 9.5856\n",
            "Epoch [8/10], Step [520/805], Loss: 9.1588\n",
            "Epoch [8/10], Step [530/805], Loss: 6.8784\n",
            "Epoch [8/10], Step [540/805], Loss: 8.8828\n",
            "Epoch [8/10], Step [550/805], Loss: 6.8521\n",
            "Epoch [8/10], Step [560/805], Loss: 10.2641\n",
            "Epoch [8/10], Step [570/805], Loss: 8.8016\n",
            "Epoch [8/10], Step [580/805], Loss: 10.8377\n",
            "Epoch [8/10], Step [590/805], Loss: 8.0976\n",
            "Epoch [8/10], Step [600/805], Loss: 9.4111\n",
            "Epoch [8/10], Step [610/805], Loss: 6.8363\n",
            "Epoch [8/10], Step [620/805], Loss: 10.7456\n",
            "Epoch [8/10], Step [630/805], Loss: 13.6290\n",
            "Epoch [8/10], Step [640/805], Loss: 16.6017\n",
            "Epoch [8/10], Step [650/805], Loss: 13.8528\n",
            "Epoch [8/10], Step [660/805], Loss: 16.4306\n",
            "Epoch [8/10], Step [670/805], Loss: 9.1428\n",
            "Epoch [8/10], Step [680/805], Loss: 7.6412\n",
            "Epoch [8/10], Step [690/805], Loss: 13.6640\n",
            "Epoch [8/10], Step [700/805], Loss: 9.9164\n",
            "Epoch [8/10], Step [710/805], Loss: 7.0703\n",
            "Epoch [8/10], Step [720/805], Loss: 6.4628\n",
            "Epoch [8/10], Step [730/805], Loss: 9.0349\n",
            "Epoch [8/10], Step [740/805], Loss: 12.2532\n",
            "Epoch [8/10], Step [750/805], Loss: 8.8961\n",
            "Epoch [8/10], Step [760/805], Loss: 6.7953\n",
            "Epoch [8/10], Step [770/805], Loss: 10.0913\n",
            "Epoch [8/10], Step [780/805], Loss: 9.7165\n",
            "Epoch [8/10], Step [790/805], Loss: 5.6683\n",
            "Epoch [8/10], Step [800/805], Loss: 9.0993\n",
            "Epoch [8/10], Average Loss: 9.4215\n",
            "Epoch [9/10], Step [0/805], Loss: 9.0451\n",
            "Epoch [9/10], Step [10/805], Loss: 9.9406\n",
            "Epoch [9/10], Step [20/805], Loss: 5.4651\n",
            "Epoch [9/10], Step [30/805], Loss: 8.8107\n",
            "Epoch [9/10], Step [40/805], Loss: 10.6002\n",
            "Epoch [9/10], Step [50/805], Loss: 6.0348\n",
            "Epoch [9/10], Step [60/805], Loss: 7.3869\n",
            "Epoch [9/10], Step [70/805], Loss: 9.5866\n",
            "Epoch [9/10], Step [80/805], Loss: 7.6605\n",
            "Epoch [9/10], Step [90/805], Loss: 6.7493\n",
            "Epoch [9/10], Step [100/805], Loss: 8.8071\n",
            "Epoch [9/10], Step [110/805], Loss: 4.5545\n",
            "Epoch [9/10], Step [120/805], Loss: 7.4851\n",
            "Epoch [9/10], Step [130/805], Loss: 11.8891\n",
            "Epoch [9/10], Step [140/805], Loss: 13.7333\n",
            "Epoch [9/10], Step [150/805], Loss: 14.1098\n",
            "Epoch [9/10], Step [160/805], Loss: 10.0275\n",
            "Epoch [9/10], Step [170/805], Loss: 7.8146\n",
            "Epoch [9/10], Step [180/805], Loss: 7.5415\n",
            "Epoch [9/10], Step [190/805], Loss: 5.5435\n",
            "Epoch [9/10], Step [200/805], Loss: 9.6242\n",
            "Epoch [9/10], Step [210/805], Loss: 7.4155\n",
            "Epoch [9/10], Step [220/805], Loss: 9.6203\n",
            "Epoch [9/10], Step [230/805], Loss: 8.2754\n",
            "Epoch [9/10], Step [240/805], Loss: 7.1082\n",
            "Epoch [9/10], Step [250/805], Loss: 9.1190\n",
            "Epoch [9/10], Step [260/805], Loss: 8.6432\n",
            "Epoch [9/10], Step [270/805], Loss: 9.3003\n",
            "Epoch [9/10], Step [280/805], Loss: 7.5113\n",
            "Epoch [9/10], Step [290/805], Loss: 8.0495\n",
            "Epoch [9/10], Step [300/805], Loss: 6.6320\n",
            "Epoch [9/10], Step [310/805], Loss: 14.5260\n",
            "Epoch [9/10], Step [320/805], Loss: 6.9675\n",
            "Epoch [9/10], Step [330/805], Loss: 8.4861\n",
            "Epoch [9/10], Step [340/805], Loss: 10.7497\n",
            "Epoch [9/10], Step [350/805], Loss: 9.5981\n",
            "Epoch [9/10], Step [360/805], Loss: 5.2264\n",
            "Epoch [9/10], Step [370/805], Loss: 12.0171\n",
            "Epoch [9/10], Step [380/805], Loss: 9.0560\n",
            "Epoch [9/10], Step [390/805], Loss: 11.6291\n",
            "Epoch [9/10], Step [400/805], Loss: 10.0152\n",
            "Epoch [9/10], Step [410/805], Loss: 11.9674\n",
            "Epoch [9/10], Step [420/805], Loss: 8.6405\n",
            "Epoch [9/10], Step [430/805], Loss: 8.6860\n",
            "Epoch [9/10], Step [440/805], Loss: 10.1879\n",
            "Epoch [9/10], Step [450/805], Loss: 10.6775\n",
            "Epoch [9/10], Step [460/805], Loss: 7.7869\n",
            "Epoch [9/10], Step [470/805], Loss: 6.6215\n",
            "Epoch [9/10], Step [480/805], Loss: 9.9302\n",
            "Epoch [9/10], Step [490/805], Loss: 10.7039\n",
            "Epoch [9/10], Step [500/805], Loss: 12.4259\n",
            "Epoch [9/10], Step [510/805], Loss: 6.5458\n",
            "Epoch [9/10], Step [520/805], Loss: 8.8223\n",
            "Epoch [9/10], Step [530/805], Loss: 10.0159\n",
            "Epoch [9/10], Step [540/805], Loss: 6.6306\n",
            "Epoch [9/10], Step [550/805], Loss: 9.9286\n",
            "Epoch [9/10], Step [560/805], Loss: 7.8093\n",
            "Epoch [9/10], Step [570/805], Loss: 9.2828\n",
            "Epoch [9/10], Step [580/805], Loss: 8.9629\n",
            "Epoch [9/10], Step [590/805], Loss: 14.1745\n",
            "Epoch [9/10], Step [600/805], Loss: 17.1053\n",
            "Epoch [9/10], Step [610/805], Loss: 6.8030\n",
            "Epoch [9/10], Step [620/805], Loss: 12.6134\n",
            "Epoch [9/10], Step [630/805], Loss: 5.2709\n",
            "Epoch [9/10], Step [640/805], Loss: 7.4820\n",
            "Epoch [9/10], Step [650/805], Loss: 9.7251\n",
            "Epoch [9/10], Step [660/805], Loss: 8.4215\n",
            "Epoch [9/10], Step [670/805], Loss: 11.6673\n",
            "Epoch [9/10], Step [680/805], Loss: 6.8463\n",
            "Epoch [9/10], Step [690/805], Loss: 7.9896\n",
            "Epoch [9/10], Step [700/805], Loss: 5.8409\n",
            "Epoch [9/10], Step [710/805], Loss: 10.6177\n",
            "Epoch [9/10], Step [720/805], Loss: 3.8367\n",
            "Epoch [9/10], Step [730/805], Loss: 9.1651\n",
            "Epoch [9/10], Step [740/805], Loss: 13.1848\n",
            "Epoch [9/10], Step [750/805], Loss: 9.2910\n",
            "Epoch [9/10], Step [760/805], Loss: 7.8710\n",
            "Epoch [9/10], Step [770/805], Loss: 10.4769\n",
            "Epoch [9/10], Step [780/805], Loss: 9.0486\n",
            "Epoch [9/10], Step [790/805], Loss: 5.3339\n",
            "Epoch [9/10], Step [800/805], Loss: 11.6926\n",
            "Epoch [9/10], Average Loss: 8.9736\n",
            "Epoch [10/10], Step [0/805], Loss: 6.5134\n",
            "Epoch [10/10], Step [10/805], Loss: 6.5515\n",
            "Epoch [10/10], Step [20/805], Loss: 7.8089\n",
            "Epoch [10/10], Step [30/805], Loss: 7.5871\n",
            "Epoch [10/10], Step [40/805], Loss: 8.8849\n",
            "Epoch [10/10], Step [50/805], Loss: 6.1820\n",
            "Epoch [10/10], Step [60/805], Loss: 11.0638\n",
            "Epoch [10/10], Step [70/805], Loss: 10.6538\n",
            "Epoch [10/10], Step [80/805], Loss: 6.8539\n",
            "Epoch [10/10], Step [90/805], Loss: 8.2533\n",
            "Epoch [10/10], Step [100/805], Loss: 9.0016\n",
            "Epoch [10/10], Step [110/805], Loss: 9.7171\n",
            "Epoch [10/10], Step [120/805], Loss: 4.6712\n",
            "Epoch [10/10], Step [130/805], Loss: 8.5227\n",
            "Epoch [10/10], Step [140/805], Loss: 7.6495\n",
            "Epoch [10/10], Step [150/805], Loss: 9.3580\n",
            "Epoch [10/10], Step [160/805], Loss: 9.2369\n",
            "Epoch [10/10], Step [170/805], Loss: 10.6144\n",
            "Epoch [10/10], Step [180/805], Loss: 14.2725\n",
            "Epoch [10/10], Step [190/805], Loss: 10.9649\n",
            "Epoch [10/10], Step [200/805], Loss: 11.9062\n",
            "Epoch [10/10], Step [210/805], Loss: 7.6133\n",
            "Epoch [10/10], Step [220/805], Loss: 7.0225\n",
            "Epoch [10/10], Step [230/805], Loss: 6.8380\n",
            "Epoch [10/10], Step [240/805], Loss: 4.7775\n",
            "Epoch [10/10], Step [250/805], Loss: 9.7071\n",
            "Epoch [10/10], Step [260/805], Loss: 9.0545\n",
            "Epoch [10/10], Step [270/805], Loss: 8.0073\n",
            "Epoch [10/10], Step [280/805], Loss: 6.4768\n",
            "Epoch [10/10], Step [290/805], Loss: 7.0748\n",
            "Epoch [10/10], Step [300/805], Loss: 10.8931\n",
            "Epoch [10/10], Step [310/805], Loss: 9.0018\n",
            "Epoch [10/10], Step [320/805], Loss: 8.7161\n",
            "Epoch [10/10], Step [330/805], Loss: 7.1252\n",
            "Epoch [10/10], Step [340/805], Loss: 7.3795\n",
            "Epoch [10/10], Step [350/805], Loss: 7.3089\n",
            "Epoch [10/10], Step [360/805], Loss: 8.7181\n",
            "Epoch [10/10], Step [370/805], Loss: 11.5003\n",
            "Epoch [10/10], Step [380/805], Loss: 6.0194\n",
            "Epoch [10/10], Step [390/805], Loss: 4.4719\n",
            "Epoch [10/10], Step [400/805], Loss: 11.2498\n",
            "Epoch [10/10], Step [410/805], Loss: 10.6265\n",
            "Epoch [10/10], Step [420/805], Loss: 6.3727\n",
            "Epoch [10/10], Step [430/805], Loss: 6.3409\n",
            "Epoch [10/10], Step [440/805], Loss: 11.5324\n",
            "Epoch [10/10], Step [450/805], Loss: 8.8253\n",
            "Epoch [10/10], Step [460/805], Loss: 11.1704\n",
            "Epoch [10/10], Step [470/805], Loss: 7.4038\n",
            "Epoch [10/10], Step [480/805], Loss: 6.7353\n",
            "Epoch [10/10], Step [490/805], Loss: 9.0863\n",
            "Epoch [10/10], Step [500/805], Loss: 7.6274\n",
            "Epoch [10/10], Step [510/805], Loss: 5.7630\n",
            "Epoch [10/10], Step [520/805], Loss: 10.0432\n",
            "Epoch [10/10], Step [530/805], Loss: 11.1723\n",
            "Epoch [10/10], Step [540/805], Loss: 10.8993\n",
            "Epoch [10/10], Step [550/805], Loss: 8.2578\n",
            "Epoch [10/10], Step [560/805], Loss: 7.8092\n",
            "Epoch [10/10], Step [570/805], Loss: 6.0161\n",
            "Epoch [10/10], Step [580/805], Loss: 13.1279\n",
            "Epoch [10/10], Step [590/805], Loss: 9.0762\n",
            "Epoch [10/10], Step [600/805], Loss: 9.4060\n",
            "Epoch [10/10], Step [610/805], Loss: 8.9124\n",
            "Epoch [10/10], Step [620/805], Loss: 7.3268\n",
            "Epoch [10/10], Step [630/805], Loss: 9.2761\n",
            "Epoch [10/10], Step [640/805], Loss: 7.8497\n",
            "Epoch [10/10], Step [650/805], Loss: 9.5465\n",
            "Epoch [10/10], Step [660/805], Loss: 10.3252\n",
            "Epoch [10/10], Step [670/805], Loss: 6.8825\n",
            "Epoch [10/10], Step [680/805], Loss: 9.5334\n",
            "Epoch [10/10], Step [690/805], Loss: 12.6948\n",
            "Epoch [10/10], Step [700/805], Loss: 7.1127\n",
            "Epoch [10/10], Step [710/805], Loss: 7.7849\n",
            "Epoch [10/10], Step [720/805], Loss: 6.5406\n",
            "Epoch [10/10], Step [730/805], Loss: 11.1560\n",
            "Epoch [10/10], Step [740/805], Loss: 7.4265\n",
            "Epoch [10/10], Step [750/805], Loss: 10.4089\n",
            "Epoch [10/10], Step [760/805], Loss: 11.4818\n",
            "Epoch [10/10], Step [770/805], Loss: 5.8165\n",
            "Epoch [10/10], Step [780/805], Loss: 8.2694\n",
            "Epoch [10/10], Step [790/805], Loss: 5.8071\n",
            "Epoch [10/10], Step [800/805], Loss: 4.6362\n",
            "Epoch [10/10], Average Loss: 8.6816\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_idx, (images, targets) in enumerate(train_loader):\n",
        "        images, targets = images.to(device), targets.to(device)  # Move both images and targets to device\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = yolo_loss(outputs, targets, S=S, B=B)  # Ensure yolo_loss handles device tensors\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "torch.save(model.state_dict(), \"yolov1_widerface_model.pth\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}